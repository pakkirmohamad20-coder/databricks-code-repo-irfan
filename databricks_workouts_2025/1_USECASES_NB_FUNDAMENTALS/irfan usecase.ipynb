{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d709c4d7-ad71-4428-af7b-6fa8f465eb1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE VOLUME IF NOT EXISTS workspace.default.usage_metrics;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e0174b4-91b7-43bd-94bf-628b0ac05ebf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üöÄ Project Title: Sample Data Engineering Task\n",
    "\n",
    "## üìã Task Description\n",
    "This notebook demonstrates how to **prepare, clean, and save data** into a Delta table.  \n",
    "We will focus on ensuring schema compatibility, handling invalid column names, and applying best practices for PySpark workflows.  \n",
    "\n",
    "---\n",
    "\n",
    "## üë©‚Äçüíª Author\n",
    "<span style=\"color:blue\">**Pakkir_Fathima**</span>\n",
    "\n",
    "---\n",
    "\n",
    "## üñºÔ∏è Our Team\n",
    "![Team Photo](https://fpimages.withfloats.com/actual/6929d1ac956d0a744b5c9822.jpeg)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ú® Notes\n",
    "- Use **bold** for emphasis.  \n",
    "- Use *italics* for highlighting important terms.  \n",
    "- Headings (`#`, `##`, `###`) organize the notebook clearly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2173ae15-c0e3-480b-aa99-5608a9f1fbed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "## üíª Step 1: Install and Import the necessary libraries\n",
    "\n",
    "# In most modern Databricks runtimes, 'requests' is already available.\n",
    "# If not, you would run a command like:\n",
    "# %pip install requests\n",
    "import requests\n",
    "print(\"Libraries imported successfully.\")\n",
    "\n",
    "# COMMAND ----------\n",
    "## üöÄ Step 2: Define URL and Fetch Data using requests\n",
    "\n",
    "# Define the source URL\n",
    "data_url = \"https://public.tableau.com/app/sample-data/mobile_os_usage.csv\"\n",
    "\n",
    "# Perform the API call using the requests library\n",
    "# A variable named 'response' will be created in the Python environment\n",
    "response = requests.get(data_url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    print(f\"Successfully fetched data from: {data_url}\")\n",
    "else:\n",
    "    print(f\"Error fetching data. Status Code: {response.status_code}\")\n",
    "    raise Exception(\"Failed to fetch data.\")\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "## üíæ Step 3: Write the Data to the Volume using dbutils.fs.put\n",
    "\n",
    "# Define the target path in the volume\n",
    "volume_path = \"/Volumes/workspace/default/usage_metrics/mobile_os_usage.csv\"\n",
    "\n",
    "# Use dbutils.fs.put to write the content of the response.text\n",
    "try:\n",
    "    # 'response' variable is accessible because the notebook language is Python\n",
    "    dbutils.fs.put(volume_path, response.text, overwrite=True)\n",
    "    print(f\"Data successfully written to Volume at: {volume_path}\")\n",
    "except Exception as e:\n",
    "    # This exception handler will catch Volume-related errors (permissions, path not found)\n",
    "    print(f\"Error writing data to Volume: {e}\")\n",
    "    raise e\n",
    "\n",
    "# COMMAND ----------\n",
    "## ‚úÖ Step 4: Verify the file was written (Optional)\n",
    "# This will list the contents of the directory to confirm the file exists\n",
    "print(\"\\nVerifying file creation:\")\n",
    "dbutils.fs.ls(\"/Volumes/workspace/default/usage_metrics/\")\n",
    "\n",
    "# COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bb9ea56-d3c0-4e06-bbeb-dae2883d0e1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run\n",
    "\"/Workspace/Users/pakkirmohamad25@gmail.com/databricks-code-repo-irfan/databricks_workouts_2025/1_DATABRICKS_NOTEBOOK_FUNDAMENTALS/4_child_notebook\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61cb4c09-756b-4163-936c-a339f20a30ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "ls /Volumes/workspace/default/usage_metrics\n",
    "head /Volumes/workspace/default/usage_metrics/mobile_os_usage.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa6c5ed9-ef26-426e-8ccf-fda29d92ecfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %python\n",
    "# Read the CSV file from the volume into a PySpark DataFrame\n",
    "df1 = spark.read.option(\"header\", \"true\").csv(\"/Volumes/workspace/default/usage_metrics/mobile_os_usage.csv\")\n",
    "\n",
    "# Show the first few rows\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f20dd22-0f51-405f-befe-d8f3c882bbb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %python\n",
    "# Read the CSV file into a PySpark DataFrame\n",
    "df1 = spark.read.option(\"header\", \"true\").csv(\"/Volumes/workspace/default/usage_metrics/mobile_os_usage.csv\")\n",
    "\n",
    "# Rename columns to remove spaces\n",
    "df1 = df1.withColumnRenamed(\"Mobile Operating System\", \"Mobile_Operating_System\") \\\n",
    "         .withColumnRenamed(\"Percent of Usage\", \"Percent_of_Usage\")\n",
    "\n",
    "# Show the first few rows\n",
    "df1.show()\n",
    "\n",
    "# Write the DataFrame into a Databricks table\n",
    "df1.write.mode(\"overwrite\").saveAsTable(\"default.mobile_os_usage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f57634f7-f84a-48fc-9d6d-27fe087dc294",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from mobile_os_usage;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbc4945c-4b44-4ea0-bb3e-5c5d7b304d2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "## üíª Step 1: Define the Python Function\n",
    "\n",
    "def convert_to_uppercase(input_string):\n",
    "  \"\"\"\n",
    "  Converts the input string to uppercase using the built-in .upper() method.\n",
    "  \"\"\"\n",
    "  # Check if the input is a string before attempting the conversion\n",
    "  if isinstance(input_string, str):\n",
    "    return input_string.upper()\n",
    "  else:\n",
    "    # Handle non-string input gracefully\n",
    "    return str(input_string).upper()\n",
    "\n",
    "\n",
    "\n",
    "## üöÄ Step 2: Use the Magic Command to Execute the Function\n",
    "\n",
    "# Define inputs\n",
    "text_1 = \"This is a test sentence.\"\n",
    "text_2 = \"PySpark is fun.\"\n",
    "\n",
    "# Call the function and print the results\n",
    "result_1 = convert_to_uppercase(text_1)\n",
    "result_2 = convert_to_uppercase(text_2)\n",
    "\n",
    "print(f\"Original 1: '{text_1}'\")\n",
    "print(f\"Uppercase 1: '{result_1}'\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"Original 2: '{text_2}'\")\n",
    "print(f\"Uppercase 2: '{result_2}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa076cc5-91fe-4603-94ba-0e2e7b8e1bdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10c169ab-c662-4d22-af7f-343261c54b77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "## üíª Step 1: Import Pandas and define file path\n",
    "\n",
    "# Ensure pandas is installed if necessary (usually pre-installed in Databricks)\n",
    "# %pip install pandas\n",
    "\n",
    "import pandas as pd\n",
    "print(\"Pandas imported successfully.\")\n",
    "\n",
    "# Replace this with the actual path to your CSV file in DBFS or a Volume\n",
    "# Example path to the data we used previously:\n",
    "file_path = \"/Volumes/workspace/default/usage_metrics/mobile_os_usage.csv\"\n",
    "\n",
    "\n",
    "## üöÄ Step 2: Read CSV and Display Output\n",
    "\n",
    "# Use the pandas read_csv function\n",
    "try:\n",
    "    df_pandas = pd.read_csv(file_path)\n",
    "\n",
    "    # Display the output. \n",
    "    # In Databricks, simply outputting the pandas DataFrame variable\n",
    "    # often triggers a rich HTML display in the notebook output.\n",
    "    print(\"--- First 5 Rows of Data ---\")\n",
    "    display(df_pandas.head())\n",
    "\n",
    "    print(\"\\n--- DataFrame Information ---\")\n",
    "    df_pandas.info()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    print(\"Please ensure the file path is correct and the file exists.\")\n",
    "\n",
    "# COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6f55c9c-3860-47c1-bd20-899554c80602",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "echo \"Magic commands tasks completed\"\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6943680436049181,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "irfan usecase",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
